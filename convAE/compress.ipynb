{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**COMPRESS TIME SERIES**"
      ],
      "metadata": {
        "id": "LaLPrCOCNIw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn3n2anUM_EZ"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "from sys import argv\n",
        "from csv import reader\n",
        "\n",
        "import keras\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import models, layers,optimizers, losses, metrics\n",
        "from keras.models import Model,load_model\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization, Conv1D, MaxPooling1D, UpSampling1D\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions to read and preproccess data"
      ],
      "metadata": {
        "id": "ApthJYZ-NSEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(dataset_path):\n",
        "    dataset=open(dataset_path,\"r\")\n",
        "    train_data = []\n",
        "    names = []\n",
        "    with open(dataset_path, 'r') as read_obj:\n",
        "        for row in dataset:\n",
        "            serie=row.split()\n",
        "            names.append(serie.pop(0)) #save the names of the series \n",
        "            serie=[float(x) for x in serie]\n",
        "            train_data.append(np.array(serie))\n",
        "    return train_data, names\n",
        "\n",
        "def scale_data(X):\n",
        "    X=np.vstack(X)\n",
        "    scaler = MinMaxScaler()\n",
        "    X=scaler.fit_transform(X)\n",
        "    return X\n",
        "\n",
        "def preprocces_data(data):\n",
        "    x=data.shape[0]\n",
        "    y=data.shape[1]\n",
        "    X = data.reshape((x,y, 1))\n",
        "    return X,y"
      ],
      "metadata": {
        "id": "9kZ2h__qNOUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define file paths (given as arguments)\n",
        "*change the file names to the ones you want to run. "
      ],
      "metadata": {
        "id": "SfZkSvBnNVyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #parse comand line arguments\n",
        "# dataset_path= ''\n",
        "# for index, argument in enumerate(argv):\n",
        "#     if argument == '-d': dataset_path = argv[index+1]\n",
        "#     if argument == '-od': input_out_path = argv[index+1]\n",
        "#     if argument == '-oq': query_out_path = argv[index+1]\n",
        "\n",
        "# if not dataset_path: dataset_path =input('Please provide the file path of the input/query data: ')\n",
        "# if not input_out_path: input_out_path = input('Please provide the path to store the encoded input data: ')\n",
        "# if not query_out_path: query_out_path = input('Please provide the path to store the encoded query data: ')\n",
        "# model_path = input('Please provide the path of the training model: ')\n",
        "\n",
        "dataset_path = 'nasdaq2007_17.csv'\n",
        "input_out_path = 'input_encoded.csv'\n",
        "query_out_path = 'query_encoded.csv'\n",
        "model_path ='second_best_model.h5'"
      ],
      "metadata": {
        "id": "K034BGnFNWFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proccess data, load model and compress"
      ],
      "metadata": {
        "id": "mMafeI0fJP1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read and prepare data\n",
        "data,names = read_data(dataset_path) #read file\n",
        "data = scale_data(data) #scale file data\n",
        "X,y = preprocces_data(data)#reshape data\n",
        "\n",
        "#load encoder\n",
        "AE = load_model(model_path)\n",
        "encoder_output = AE.get_layer('bottleneck_layer').output\n",
        "\n",
        "#build a model\n",
        "encoder_model = keras.Model(AE.input,encoder_output)\n",
        "\n",
        "encoded_data = encoder_model.predict(X) #encode all data"
      ],
      "metadata": {
        "id": "18PMKgT4Bab0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save to corresponding files"
      ],
      "metadata": {
        "id": "jldGCpgHJNQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = np.column_stack((names, encoded_data))#merge encoded timeseries with their names\n",
        "input,query = np.split(final_data,[349]) #split the data in input and qywry timeseries to be used in the project2 (349/10)\n",
        "print(\"input: \",input.shape)\n",
        "print(\"query: \",query.shape)\n",
        "#save on csv files\n",
        "np.savetxt(\"input.csv\", input ,fmt='%s', delimiter=\"\\t\")\n",
        "np.savetxt(\"query.csv\", query ,fmt='%s', delimiter=\"\\t\")\n",
        "\n",
        "encoder_model.summary()#print model summary"
      ],
      "metadata": {
        "id": "dE5C3oS3XcBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}